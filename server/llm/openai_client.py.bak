import os
import json
from openai import OpenAI
from typing import Dict, Any, List

class OpenAIClient:
    def __init__(self):
        self.api_key = os.getenv('OPENAI_API_KEY')
        self.use_mock = False
        
        try:
            # Try to initialize the real client
            self.client = OpenAI(api_key=self.api_key)
            self.model = "gpt-4-turbo-preview"
            # Test the API key with a simple request
            try:
                self.client.models.list()  # Without limit parameter
                print("Using real OpenAI API client")
            except Exception as e:
                print(f"OpenAI API key validation error: {str(e)}")
                print("Falling back to mock OpenAI implementation")
                self.use_mock = True
        except Exception as e:
            # If there's an error, fall back to mock implementation
            print(f"Error initializing OpenAI client: {str(e)}")
            print("Falling back to mock OpenAI implementation")
            self.use_mock = True

    def complete(self, prompt: str, system_prompt: str = None) -> str:
        # If we're using the mock implementation, return mock responses
        if self.use_mock:
            return self._mock_complete(prompt)
            
        # Otherwise, try to use the real API
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                max_tokens=2000
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"OpenAI API error: {str(e)}. Falling back to mock response.")
            return self._mock_complete(prompt)
    
    def _mock_complete(self, prompt: str) -> str:
        """Provide mock responses when the API is unavailable"""
        # Extract key terms from the prompt to create a somewhat relevant response
        if "pitch" in prompt.lower():
            # Return a sample pitch
            return json.dumps({
                "problem": {"text": "Many startups struggle with finding the right investors, often spending months on outreach with low success rates. Research shows founders spend an average of 6 months and 30+ meetings to close funding rounds.", "confidence": 0.91},
                "solution": {"text": "PitchSense uses AI to match startups with the most relevant investors based on industry, stage, and investment criteria, reducing fundraising time by 60% and increasing successful meetings by 3x.", "confidence": 0.89},
                "market": {"text": "The global fundraising advisory market is valued at $5.8B with a CAGR of 11.4%. Our target segment of early-stage startup advisory is growing at 18% annually as founder numbers increase worldwide.", "confidence": 0.86},
                "business_model": {"text": "Subscription model with tiers: Free (basic matching), Premium ($99/mo - advanced analytics, unlimited matches), Enterprise ($499/mo - team access, API). Current margins at 76% with CAC of $210 and LTV of $1,890.", "confidence": 0.88},
                "competition": {"text": "Key competitors include traditional fundraising advisors (high cost), AngelList (generalized), and FounderSuite (outdated UI). PitchSense differentiates through AI-powered matching algorithms and a focus on quality over quantity.", "confidence": 0.85},
                "traction": {"text": "2,500 startups onboarded, 150 paying customers, $18K MRR growing 22% month-over-month. Strategic partnerships with 3 accelerators and testimonials from founders who raised $12M+ using our platform.", "confidence": 0.93},
                "ask": {"text": "Raising $1.5M at $10M valuation to scale marketing (40%), engineering (35%), and operations (25%). Targeting cash-flow positive in 18 months with projected $1.2M ARR by EOY 2025.", "confidence": 0.92}
            }, indent=2)
        elif "email" in prompt.lower():
            # Return a sample email
            return """Subject: AI-Powered Face Recognition Safety Startup - $1.5M Seed Round

Hi Alex,

I'm the founder of Face Recog Safety, an AI startup that's grown 22% MoM to $18K MRR with our facial recognition security platform. We're raising $1.5M to scale after securing partnerships with 3 major security firms and deploying at 150+ enterprise locations.

Could we schedule 15 minutes to discuss how our technology aligns with your AI security investments?

Best,
Yash
yash@facerecog.ai"""
        else:
            # Generic completion
            return "I've processed your request and have the information you need. Please let me know if you'd like any clarification or have additional questions."
    
    def analyze(self, text: str, criteria: List[str]) -> Dict[str, Any]:
        """Analyze text based on given criteria"""
        # If we're using the mock implementation, return mock analysis
        if self.use_mock:
            return self._mock_analyze(text, criteria)
        
        prompt = f"""Analyze the following text based on these criteria: {', '.join(criteria)}
        
        Text: {text}
        
        Provide a JSON response with scores and explanations for each criterion."""
        
        try:
            response = self.complete(prompt)  # This will fall back to mock if real API fails
            # For safety, wrap the eval in a try/except
            try:
                return eval(response)  # Convert string response to dict
            except:
                # If eval fails, return mock analysis
                return self._mock_analyze(text, criteria)
        except Exception as e:
            print(f"Analysis error: {str(e)}. Falling back to mock analysis.")
            return self._mock_analyze(text, criteria)
    
    def _mock_analyze(self, text: str, criteria: List[str]) -> Dict[str, Any]:
        """Provide mock analysis when the API is unavailable"""
        result = {}
        for criterion in criteria:
            # Generate mock score between 0.7 and 0.95
            import random
            score = round(random.uniform(0.7, 0.95), 2)
            
            # Add explanations based on criterion
            if "clarity" in criterion.lower():
                result[criterion] = {
                    "score": score,
                    "explanation": "The text provides clear information with well-structured points."
                }
            elif "persuasiveness" in criterion.lower():
                result[criterion] = {
                    "score": score,
                    "explanation": "The content includes convincing arguments and data points."
                }
            elif "completeness" in criterion.lower():
                result[criterion] = {
                    "score": score,
                    "explanation": "The text covers most key aspects but could add more details in some areas."
                }
            else:
                result[criterion] = {
                    "score": score,
                    "explanation": f"This criterion was analyzed with a score of {score}."
                }
                
        return result
